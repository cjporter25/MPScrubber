***SETUP***
    - Created a main folder called "MPScrubber"
    - Added a base python file called "app.py"
    - Added this text file to notate active steps taken during development
    - Initialized a virtual environment where all project libraries, necessary
      for it to function, will be installed.
        - ACTIVATION: .venv/Scripts/Activate.ps1

***FIRST STEPS***
    - To create a more realistic initial testing environment, this project will 
      focus on Facebook Marketplace as its URL schema seems to be much cleaner than
      Craigslist
    - Before this, I need to learn how to conduct basic web scrapping
    - ***NECESSARY IMPORTS***
        - "requests"
            - This module allows python to make a request on the web given a valid URL
            - "pip install requests"
        - "beautifulSoup"
            - This module allows python to cleanly read-in raw HTML that can then be
              sifted through.
            - "pip install beautifulsoup4"
        - "selenium"
            - This module allows for an anonymous usage of a browser's tools
            - "pip install selenium"
        - "options"
            - Python package that allows for the creation of an "options" area when initializing
              a browser
            - "pip install options"
        - "webdriver-manager"
            - Aids Selenium by automatically importing specific browser drivers to be used when needed
            - "pip install webdriver-manager"
    - ***KEEPING TRACK OF IMPORTS***
        - pip freeze
            - This command collects all currently installed libraries. This collection can
              then be output to some file. In this case, the file is called "requirements.txt"
            - "pip freeze > requirements.txt"
                        OR 
            - "pip freeze > logs/requirements.txt" (If in another folder)

***LEARNING REQUESTS***
    - The requests library in python allows the user to make requests to a webpage and
      return a "requests.Response" object
        - "requests.get" --> requests.get(url, params={key: value}, args)
    - Request Response Objects
        - Contain a variety of properties and methods. For this application, the object's
          property, "text", is what contains the content of the response. (in unicode)

***LEARNING BEAUTIFUL SOUP***
    - EXAMPLE HTML
        - """<html>
              <head>
                <title>The Dormouse's story</title>
              </head>
              <body>
                <p class="title">
                  <b>The Dormouse's story</b>
                </p>
                <p class="story">Once upon a time there were three little sisters; and their names were
                  <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
                  <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a>, and
                  <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
                and they lived at the bottom of a well.</p>

                <p class="story">
                  ...
                </p>
          """</html>
    - Method: Find all elements of a specific attribute type
        - "soup.find_all('a')"
          - [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
             <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
             <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
    - Method: Find a single element of a specified ID
        - "soup.find(id="link3")
          - <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>
    - Method: Retrieve all of the text from a page (non-HTML format):
        - "soup.get_text()"


***FIRST SUCCESSFUL PULL***
    - (12.19.23): With the help of some guidance to use a browser driver called Selenium,
      I was able to successfully load the page, scrape the descriptions of each post, and store
      them in a separate text file.

***RESOLVED ERRORING OUT***
    - (1.7.23): By forcing a maximum amount of data that can be pulled, i.e. 20 postings per run 
      through, the program no longer comes across and HTML without any data.

***RESOLVED OUTPUT ISSUE***
    - (1.7.23): When outputting to the test text file, to ensure I could actually retrieve the right
      data, I noticed the HTML for location and mileage were the same. Once I used the "find_all" 
      method, it pulled in both data points into a list. Since they're always in order, the program 
      simply pulls the text from the [0]'th and [1]'st position (location and mileage) 

***FLESHED OUT FACEBOOK PYTHON MODULE***
    - (1.15.23): The facebook module is now setup to handle all page_source data management, as 
      well as became the main facilitator and connection to the new facebookDB. Due to how the sorting
      functions work on the website itself/how cars are often organized, the tables in the database
      are unique to each brand of car, i.e., a page retrieving Toyota branded vehicles will be stored
      in the Toyota table.

***EDGE CASES AND EMPTY RETRIEVALS***
    - (2.11.23): During the development process, it has become apparent that the websites we're scrubbing
      from will likely periodically change the attribute tags for specific elements I might be scrubbing 
      for. For example, the last two weeks, I've been unable to retrieve the link attached to each post.
      I discovered that the "href" element tag had changed. I manually updated that and it worked!

***PREVENTED CHROME CLOSING***
    - (2.11.23):I was able to modify the launching process to no close the currently open chrome window and 
      instead open a new window. I also tried hiding this window but there were a variety of errors
      that occurred. May have to revisit in the future but it isn't critical.

***ACCOUNTING FOR ENTRIES ALREADY IN DATABASE***
    - Modified the insert entries function to accurately look for and prevent adding entries that 
      have already been added.




    